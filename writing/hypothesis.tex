We hypothetise that a fine-tuned medium size (3-7B parameters) model will perform as well as a more general purpose but larger model on a task involving multi-span abstractive QA (with provided context passages) on large input sizes (2K+ tokens).